{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classical Bo Custom LM keras.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1Gg_bjBvOd3ZEQ6oDpJvJVWzRGf1TWrrL",
      "authorship_tag": "ABX9TyOc7HAgYaL17wcZC2dSHp14",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OpenPecha-dev/models/blob/main/models/lm/Classical_Bo_Custom_LM_keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JtUOjkEST1Sf"
      },
      "outputs": [],
      "source": [
        "from typing import List, Dict\n",
        "import pickle\n",
        "\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset"
      ],
      "metadata": {
        "id": "Gfi08wzLE3n9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "def _mkdir(path: Path) -> Path:\n",
        "  path.mkdir(exist_ok=True, parents=True)\n",
        "  return path\n",
        "\n",
        "BASE_PATH = Path(\"/content/drive/MyDrive/OpenPecha/ML/LM\")\n",
        "DATA_PATH = BASE_PATH / \"data\"\n",
        "MODELS_PATH = _mkdir(BASE_PATH / \"models\" / \"BiLSTM\")"
      ],
      "metadata": {
        "id": "Ad6R281wE4-2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_paths(path) -> List[str]:\n",
        "   files = []\n",
        "   for pecha_path in tqdm(list(path.iterdir())):\n",
        "     for fn in pecha_path.iterdir():\n",
        "       if 'tokenized' not in fn.stem:\n",
        "         continue\n",
        "       files.append(fn)\n",
        "   return files\n",
        "\n",
        "def normalize_sentences_length(sentences, sent_len=50):\n",
        "  tokens = [tok for sent in sentences for tok in sent.split()]\n",
        "  sentences = []\n",
        "  for i in range(0, len(tokens), sent_len):\n",
        "    sentences.append(tokens[i: i+sent_len])\n",
        "  return [' '.join(s) for s in sentences]\n",
        "   \n",
        "def get_sentences(path, build=False):\n",
        "  sentences_fn = path / \"sentences.txt\"\n",
        "  if sentences_fn.is_file() and not build:\n",
        "    print(\"[INFO] loading sentences from last built...\")\n",
        "    for line in tqdm(sentences_fn.read_text().splitlines()):\n",
        "      if not line: continue\n",
        "      yield line\n",
        "  else:\n",
        "    print(\"[INFO] Building sentences.txt...\")\n",
        "    if sentences_fn.is_file(): sentences_fn.unlink()\n",
        "    sentences = []\n",
        "    for path in tqdm(get_text_paths(path)):\n",
        "      for line in path.read_text().splitlines():\n",
        "        if not line: continue\n",
        "        sentences.append(line)\n",
        "    sentences = normalize_sentences_length(sentences)\n",
        "    sentences_fn.write_text('\\n'.join(sentences))\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "13P9gYnlEGtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# corpus_name = \"classical_bo\"\n",
        "# corpus_path = DATA_PATH / corpus_name\n",
        "# sentences = list(get_sentences(corpus_path))\n",
        "# len(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yzbO6v0KEa1D",
        "outputId": "8edfa692-167f-4b97-da56-f191f7ab5849"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] loading sentences from last built...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64304/64304 [00:00<00:00, 694115.88it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "64304"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sentences[-5:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ExP5BgIwmT-z",
        "outputId": "45815989-6122-4607-c22a-880d9fa86cc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['༔ འོད་ ལྔ འི་ ཚུལ་ འཕྲོ ས་ སྡུག་བསྔལ་ སེལ ༔ སྐུ་ སྟོད་ གཅེ ར་བ་ རུས་པ ས་ བརྒྱན ༔ སྐུ་ སྨད་ དར་ དམར་ ཤམ་ཐབས་ མཛེས ༔ ཕྱག་ གཉིས་ མཉམ་བཞག་ མཛད་པ འི་ སྟེང་ ༔ བདུད་རྩི་ སྨན་ གྱི་ བུམ་ བཟུང་ ཐོགས ༔ ཞབས་ གཉིས་ མཉམ་པ འི་ སྟབས་ ཀྱིས་ བཞེངས ༔ ཡེ་ཤེས་ མེ་འོད་ ཀློང་',\n",
              " 'ན་ གསལ ༔ དེ་ནས་ བཟླས་པ འི་ རིམ་པ་ ནི ༔ ཐུགས་ཀ ར་ པད་ ཟླ འི་ སྟེང་ དུ་ འཇམ་དབྱངས་ དཀར་པོ་ ལོངས་སྐུ འི་ རྒྱན་ ཅན་ ཕྱག་ གཉིས་ བུམ་པ་ བདུད་རྩི ས་ གང་བ་ བསྣམས་པ འི་ ཐུགས་ ཙིཏྟ་ རིན་པོ་ཆེ འི་ ནང་ དུ ༔ ཟླ་ སྟེང་ ཧཱུྃ་ ཡིག་དཀར་པོ འི་ མཐ ར ༔ སྔགས་ ཕྲེང་ སྐར་མ འི་ ཕྲེང་བ་',\n",
              " 'ལྟར ༔ འཁོར་བ འི་ འོད་ ཀྱིས་ དོན་ གཉིས་ བྱས ༔ ཁྱད་པར་ ཟླ་ ཞུན་ ལྟ་བུ འི་ འོད ༔ དྭངས་ བསིལ་ བདུད་རྩི་ དང་ བཅས་ འཕྲོ ས ༔ རང་གཞན་ ནད་རིམས་ རྒྱུ་རྐྱེན་ བཅས ༔ ཞི་ ཞིང་ དག་ ནས་ བདེ་སྟོང་ གི ༔ ཡེ་ཤེས་ རྒྱུད་ ལ་ སྐྱེས་ ནས་ ཀྱང་ ༔ ཟག་མེད་ རྡོ་རྗེ་ ལྟ་བུ འི་ སྐུ ༔',\n",
              " 'བསྒྲེས་ རྒུད་ མེད་པ འི་ རང་བཞིན་ བསམ ༔ ཨོཾ་ མཉྫུ་ ཤྲཱི་ ཀྲོ་ དྷ་ ར་ དྷི་པ་ ན་ ཨ་ ཙ་ ཡེ་ ཧཱུྃ་ ཕཊ ༔ འབུམ་ཕྲག་ དྲུག་ གིས་ ནད་རིམས་ ཞི ༔ ཚེ་རབས་ ཀུན་ ཏུ་ ནད་མེད་པ འི ༔ བདེ་བ་ ཕུན་སུམ་ ཚོགས་ ཐོབ་ འགྱུར ༔ ས་ མ་ ཡ ༔ ལ ས་ རབ་ གླིང་པ ས་ ཀ་ རོན་',\n",
              " 'བྲག་དཀར་ རྫ་ མདུན་ དུ་ བྲིས་པ་ དགེ ། མངྒ་ ལཾ །།']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# max_len = max([len(s.split()) for s in sentences])\n",
        "# max_len"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHEovui1nJwL",
        "outputId": "c2b96349-71d2-4dfb-8104-bea6e650b457"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "50"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer = Tokenizer(lower=False)\n",
        "# tokenizer.fit_on_texts(sentences)\n",
        "# vocab = tokenizer.word_index\n",
        "# seqs = tokenizer.texts_to_sequences_generator(sentences)\n",
        "# del sentences"
      ],
      "metadata": {
        "id": "ayna1leVDAfu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # add special token to vocab\n",
        "# UNKNOWN = '<unk>'\n",
        "# PADDING = '<pad>'\n",
        "# SENT_START = '<s>'\n",
        "# SENT_END = '</s>'\n",
        "# for i, s_token in enumerate([UNKNOWN, PADDING, SENT_START, SENT_END], start=1):\n",
        "#   if s_token in vocab:\n",
        "#     continue\n",
        "#   vocab[s_token] = len(vocab) + i\n",
        "#   print(vocab[s_token])"
      ],
      "metadata": {
        "id": "xvq1saXCh-tG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def add_start_and_end_to_seqs(seqs, start, end):\n",
        "  return [[start] + seq + [end] for seq in seqs]"
      ],
      "metadata": {
        "id": "DgGtMxNblDmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DataGenerator(tf.keras.utils.Sequence):\n",
        "\n",
        "  def __init__(self, seqs, batch_size, vocab_size, shuffle=True):\n",
        "    self.X, self.Y = [], []\n",
        "    self.create_examples(seqs)\n",
        "    self.batch_size = batch_size\n",
        "    self.vocab_size = vocab_size\n",
        "\n",
        "  def create_examples(self, seqs):\n",
        "\n",
        "    def generate_xy_pairs(seq, max_len):\n",
        "      x, y = [], []\n",
        "      for i, tok_id in enumerate(seq):\n",
        "        x_padded = pad_sequences([seq[:i]], maxlen=max_len)[0]\n",
        "        x.append(x_padded)\n",
        "        y.append(tok_id)\n",
        "      return x, y\n",
        "\n",
        "    max_len = max([len(seq) for seq in seqs])\n",
        "    for seq in tqdm(seqs):\n",
        "      xs, ys = generate_xy_pairs(seq, max_len)\n",
        "      self.X += xs\n",
        "      self.Y += ys\n",
        "\n",
        "    self.X = np.array(self.X)\n",
        "    self.Y = np.array(self.Y)\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.X) // self.batch_size\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    start = index*self.batch_size\n",
        "    end = (index+1)*self.batch_size\n",
        "    X = self.X[start: end]\n",
        "    Y = self.Y[start: end]\n",
        "\n",
        "    return np.array(X), to_categorical(Y, num_classes=self.vocab_size)"
      ],
      "metadata": {
        "id": "5LggBURxAJMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train, valid = train_test_split(list(seqs), test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "b_f0C2DeKiOL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# params = {\n",
        "#     \"batch_size\": 1000,\n",
        "#     \"vocab_size\": len(vocab) + 1,\n",
        "#     \"shuffle\": True,\n",
        "#     \"max_len\": max_len\n",
        "# }"
      ],
      "metadata": {
        "id": "gMfR1DqpSUuH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# training_generator = DataGenerator(train, params[\"batch_size\"], params[\"vocab_size\"], params[\"shuffle\"])\n",
        "# validation_generator = DataGenerator(valid, params[\"batch_size\"], params[\"vocab_size\"], params[\"shuffle\"])"
      ],
      "metadata": {
        "id": "JEyu46kboSZk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0295367-c7c6-4e6b-d805-3435d0d08403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51443/51443 [01:30<00:00, 570.83it/s]\n",
            "100%|██████████| 12861/12861 [00:21<00:00, 603.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training_generator.batch_size = params[\"batch_size\"]\n",
        "# validation_generator.batch_size = params[\"batch_size\"]"
      ],
      "metadata": {
        "id": "JYIP4efoo5Vv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "NcMXLvICWDrM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_model(params):\n",
        "#   model = Sequential()\n",
        "#   model.add(Embedding(input_dim=params[\"vocab_size\"], output_dim=100, input_length=params[\"max_len\"]))\n",
        "#   model.add(LSTM(100, return_sequences=True))\n",
        "#   model.add(LSTM(100))\n",
        "#   model.add(Dense(params[\"vocab_size\"], activation='softmax'))\n",
        "#   model.compile('rmsprop', 'categorical_crossentropy')\n",
        "  \n",
        "#   return model"
      ],
      "metadata": {
        "id": "YGOgdSKAWJOY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = get_model(params)"
      ],
      "metadata": {
        "id": "M0m6SYHSXunK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train"
      ],
      "metadata": {
        "id": "Ktq5AAKzX-ik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit_generator(\n",
        "    generator=training_generator,\n",
        "    validation_data=validation_generator,\n",
        "    epochs=10,\n",
        "    use_multiprocessing=True,\n",
        "    workers=6\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e0iZSUo-X_nV",
        "outputId": "937e0e48-2646-49ab-a7f6-36e5acf8da4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:6: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
            "  \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "2572/2572 [==============================] - 772s 299ms/step - loss: 5.1513 - val_loss: 5.0683\n",
            "Epoch 2/10\n",
            "2572/2572 [==============================] - 735s 285ms/step - loss: 4.9464 - val_loss: 4.9329\n",
            "Epoch 3/10\n",
            "2572/2572 [==============================] - 732s 283ms/step - loss: 4.8317 - val_loss: 4.8416\n",
            "Epoch 4/10\n",
            "2572/2572 [==============================] - 734s 284ms/step - loss: 4.7338 - val_loss: 4.7714\n",
            "Epoch 5/10\n",
            "2572/2572 [==============================] - 771s 299ms/step - loss: 4.6564 - val_loss: 4.7314\n",
            "Epoch 6/10\n",
            "2572/2572 [==============================] - 736s 285ms/step - loss: 4.5851 - val_loss: 4.6673\n",
            "Epoch 7/10\n",
            "2572/2572 [==============================] - 737s 286ms/step - loss: 4.5121 - val_loss: 4.6075\n",
            "Epoch 8/10\n",
            "2572/2572 [==============================] - 746s 289ms/step - loss: 4.4614 - val_loss: 4.5644\n",
            "Epoch 9/10\n",
            "2572/2572 [==============================] - 781s 302ms/step - loss: 4.3997 - val_loss: 4.5288\n",
            "Epoch 10/10\n",
            "2572/2572 [==============================] - 737s 285ms/step - loss: 4.3665 - val_loss: 4.5619\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc69158b290>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_model(path, model, tokenizer=None):\n",
        "  model.save(path)\n",
        "  if tokenizer:\n",
        "    pickle.dump(tokenizer, (path / 'tokenizer.pkl').open('wb'))\n",
        "  return path\n",
        "\n",
        "def load_model(path):\n",
        "  model = tf.keras.models.load_model(path)\n",
        "  tokenizer = pickle.load((path / 'tokenizer.pkl').open('rb'))\n",
        "  return model, tokenizer"
      ],
      "metadata": {
        "id": "PFC1mvrTwsQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = MODELS_PATH / \"lstm\"\n",
        "# save_model(model_path, model, tokenizer)"
      ],
      "metadata": {
        "id": "8TeGz7D6ZE55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate"
      ],
      "metadata": {
        "id": "R2Xt0keEbigg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "l_model, l_tokenizer = load_model(model_path)"
      ],
      "metadata": {
        "id": "Zv0Ry7QoaGsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def continue_training():\n",
        "  # get setences\n",
        "  print(\"[INFO] Loading sentences...\")\n",
        "  corpus_name = \"classical_bo\"\n",
        "  corpus_path = DATA_PATH / corpus_name\n",
        "  sentences = list(get_sentences(corpus_path))\n",
        "\n",
        "  # load model\n",
        "  model_path = MODELS_PATH / \"lstm\"\n",
        "  model, tokenizer = load_model(model_path)\n",
        "\n",
        "  # Tokenize\n",
        "  print(\"[INFO] Tokenizing sentences...\")\n",
        "  vocab = tokenizer.word_index\n",
        "  seqs = tokenizer.texts_to_sequences_generator(sentences)\n",
        "  max_len = max([len(s.split()) for s in sentences])\n",
        "  del sentences\n",
        "\n",
        "  # Define Parameter\n",
        "  params = {\n",
        "    \"batch_size\": 500,\n",
        "    \"vocab_size\": len(vocab) + 1,\n",
        "    \"shuffle\": True,\n",
        "    \"max_len\": max_len\n",
        "  }\n",
        "\n",
        "  # Create Dataset\n",
        "  print(\"[INFO] Preparing training dataset...\")\n",
        "  train, valid = train_test_split(list(seqs), test_size=0.2, random_state=42)\n",
        "  training_generator = DataGenerator(train, params[\"batch_size\"], params[\"vocab_size\"], params[\"shuffle\"])\n",
        "  validation_generator = DataGenerator(valid, params[\"batch_size\"], params[\"vocab_size\"], params[\"shuffle\"])\n",
        "\n",
        "\n",
        "  # Train model\n",
        "  print(\"[INFO] Training model...\")\n",
        "  model.fit_generator(\n",
        "      generator=training_generator,\n",
        "      validation_data=validation_generator,\n",
        "      epochs=10,\n",
        "      use_multiprocessing=True,\n",
        "      workers=6\n",
        "  )\n",
        "\n",
        "  print(f\"[INFO] Model saved at: {model_path}\")\n",
        "  save_model(model_path, model, tokenizer)\n",
        "\n",
        "  return model_path, model, tokenizer"
      ],
      "metadata": {
        "id": "yZCUvIChuOjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, l_model, l_tokenizer = continue_training()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cy-to8PYwgTv",
        "outputId": "83687b26-29df-42be-ccb5-0cf32397d641"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Loading sentences...\n",
            "[INFO] loading sentences from last built...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 64304/64304 [00:00<00:00, 2343882.20it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Tokenizing sentences...\n",
            "[INFO] Preparing training dataset...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 51443/51443 [01:03<00:00, 815.14it/s]\n",
            "100%|██████████| 12861/12861 [00:16<00:00, 791.87it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[INFO] Training model...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:41: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5144/5144 [==============================] - 685s 132ms/step - loss: 4.5170 - val_loss: 4.7032\n",
            "Epoch 2/10\n",
            "5144/5144 [==============================] - 660s 128ms/step - loss: 4.5524 - val_loss: 4.7010\n",
            "Epoch 3/10\n",
            "5144/5144 [==============================] - 660s 128ms/step - loss: 4.5501 - val_loss: 4.6935\n",
            "Epoch 4/10\n",
            "5144/5144 [==============================] - 661s 128ms/step - loss: 4.5232 - val_loss: 4.6822\n",
            "Epoch 5/10\n",
            "5144/5144 [==============================] - 676s 131ms/step - loss: 4.5010 - val_loss: 4.6706\n",
            "Epoch 6/10\n",
            "5144/5144 [==============================] - 661s 128ms/step - loss: 4.4766 - val_loss: 4.6561\n",
            "Epoch 7/10\n",
            "5144/5144 [==============================] - 635s 123ms/step - loss: 4.4578 - val_loss: 4.6665\n",
            "Epoch 8/10\n",
            "5144/5144 [==============================] - 636s 123ms/step - loss: 4.4437 - val_loss: 4.6449\n",
            "Epoch 9/10\n",
            "5144/5144 [==============================] - 637s 124ms/step - loss: 4.4140 - val_loss: 4.6346\n",
            "Epoch 10/10\n",
            "5144/5144 [==============================] - 632s 123ms/step - loss: 4.3924 - val_loss: 4.6454\n",
            "[INFO] Model saved at: /content/drive/MyDrive/OpenPecha/ML/LM/models/BiLSTM/lstm\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Found untraced functions such as lstm_cell_2_layer_call_fn, lstm_cell_2_layer_call_and_return_conditional_losses, lstm_cell_3_layer_call_fn, lstm_cell_3_layer_call_and_return_conditional_losses while saving (showing 4 of 4). These functions will not be directly callable after loading.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/OpenPecha/ML/LM/models/BiLSTM/lstm/assets\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Assets written to: /content/drive/MyDrive/OpenPecha/ML/LM/models/BiLSTM/lstm/assets\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f3df7f0bf50> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n",
            "WARNING:absl:<keras.layers.recurrent.LSTMCell object at 0x7f3df7e830d0> has the same name 'LSTMCell' as a built-in Keras object. Consider renaming <class 'keras.layers.recurrent.LSTMCell'> to avoid naming conflicts when loading with `tf.keras.models.load_model`. If renaming is not possible, pass the object in the `custom_objects` parameter of the load function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sentence Score"
      ],
      "metadata": {
        "id": "KNpTx3johKw0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_xy_pairs(seq, max_len):\n",
        "    x, y = [], []\n",
        "    for i, tok_id in enumerate(seq):\n",
        "      x_padded = pad_sequences([seq[:i]], maxlen=max_len)[0]\n",
        "      x.append(x_padded)\n",
        "      y.append(tok_id)\n",
        "    return x, y "
      ],
      "metadata": {
        "id": "w-T88amF5MQt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def score_sentence(sentence, model, tokenizer):\n",
        "  seq = tokenizer.texts_to_sequences([sentence])[0]\n",
        "  x_test, y_test = generate_xy_pairs(seq, model.layers[0].input_length)\n",
        "  x_test = np.array(x_test)\n",
        "  y_test = np.array(y_test)\n",
        "  p_pred = model.predict(x_test)\n",
        "  vocab_inv = {v: k for k, v in tokenizer.word_index.items()}\n",
        "  log_p_sentence = 0\n",
        "  for i, prob in enumerate(p_pred):\n",
        "      word = vocab_inv[y_test[i]] \n",
        "      history = ' '.join([vocab_inv[w] for w in x_test[i, :] if w != 0])\n",
        "      prob_word = prob[y_test[i]]\n",
        "      log_p_sentence += np.log(prob_word)\n",
        "      print('P(w={}|h={})={}'.format(word, history, prob_word))\n",
        "  print('Prob. sentence: {}'.format(np.exp(log_p_sentence)))"
      ],
      "metadata": {
        "id": "HnAfvzc8hLjt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'བདེ་ཆེན་ པདྨ་ འཁྱིལ་བ འི་ ཕོ་བྲང་ ནས'\n",
        "score_sentence(sent, l_model, l_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yNU34Ldk0uL",
        "outputId": "ce7b6d70-f122-4690-88a8-723550f96c0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(w=བདེ་ཆེན་|h=)=0.0004911079886369407\n",
            "P(w=པདྨ་|h=བདེ་ཆེན་)=0.0012315770145505667\n",
            "P(w=འཁྱིལ་བ|h=བདེ་ཆེན་ པདྨ་)=5.0276112233405e-06\n",
            "P(w=འི་|h=བདེ་ཆེན་ པདྨ་ འཁྱིལ་བ)=0.9756913781166077\n",
            "P(w=ཕོ་བྲང་|h=བདེ་ཆེན་ པདྨ་ འཁྱིལ་བ འི་)=0.14474745094776154\n",
            "P(w=ནས|h=བདེ་ཆེན་ པདྨ་ འཁྱིལ་བ འི་ ཕོ་བྲང་)=0.29027533531188965\n",
            "Prob. sentence: 1.2466193152867404e-13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'བདེ་ཆེན་ པདྨ་ འཁྱིལ་བ གི་ ཕོ་བྲང་ ནས་'\n",
        "score_sentence(sent, l_model, l_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dNrzWAGWk98f",
        "outputId": "a1d4e20b-fae6-4170-ac4b-948a97428b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(w=བདེ་ཆེན་|h=)=0.0004911079886369407\n",
            "P(w=པདྨ་|h=བདེ་ཆེན་)=0.0012315770145505667\n",
            "P(w=འཁྱིལ་བ|h=བདེ་ཆེན་ པདྨ་)=5.0276112233405e-06\n",
            "P(w=གི་|h=བདེ་ཆེན་ པདྨ་ འཁྱིལ་བ)=1.1202250789210666e-06\n",
            "P(w=ཕོ་བྲང་|h=བདེ་ཆེན་ པདྨ་ འཁྱིལ་བ གི་)=0.06572142988443375\n",
            "P(w=ནས་|h=བདེ་ཆེན་ པདྨ་ འཁྱིལ་བ གི་ ཕོ་བྲང་)=0.0037065837532281876\n",
            "Prob. sentence: 8.298249036817588e-22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'བདེ་ཆེན་ པདྨ་ འཁྱིལ་བ་ ཡི་ ཕོ་བྲང་ ནས་'\n",
        "score_sentence(sent, l_model, l_tokenizer)"
      ],
      "metadata": {
        "id": "mnAQYu41qh89",
        "outputId": "59168cbc-c0b5-40eb-ae16-24fd4244023d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(w=བདེ་ཆེན་|h=)=0.0004911079886369407\n",
            "P(w=པདྨ་|h=བདེ་ཆེན་)=0.0012315770145505667\n",
            "P(w=འཁྱིལ་བ་|h=བདེ་ཆེན་ པདྨ་)=8.791105301497737e-07\n",
            "P(w=ཡི་|h=བདེ་ཆེན་ པདྨ་ འཁྱིལ་བ་)=0.00028916046721860766\n",
            "P(w=ཕོ་བྲང་|h=བདེ་ཆེན་ པདྨ་ འཁྱིལ་བ་ ཡི་)=0.0043965959921479225\n",
            "P(w=ནས་|h=བདེ་ཆེན་ པདྨ་ འཁྱིལ་བ་ ཡི་ ཕོ་བྲང་)=0.00020606110047083348\n",
            "Prob. sentence: 1.3929429852153917e-22\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sent = 'བདེ་ཆེན་ བདེ་ཆེན་ བདེ་ཆེན་ བདེ་ཆེན་ བདེ་ཆེན་ བདེ་ཆེན་'\n",
        "score_sentence(sent, l_model, l_tokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIt0HwlhopNJ",
        "outputId": "0d0f92ac-594a-4ed2-bf30-4b1de9617eb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "P(w=བདེ་ཆེན་|h=)=0.0004911079886369407\n",
            "P(w=བདེ་ཆེན་|h=བདེ་ཆེན་)=0.0033430515322834253\n",
            "P(w=བདེ་ཆེན་|h=བདེ་ཆེན་ བདེ་ཆེན་)=0.008468111045658588\n",
            "P(w=བདེ་ཆེན་|h=བདེ་ཆེན་ བདེ་ཆེན་ བདེ་ཆེན་)=0.015528939664363861\n",
            "P(w=བདེ་ཆེན་|h=བདེ་ཆེན་ བདེ་ཆེན་ བདེ་ཆེན་ བདེ་ཆེན་)=0.011780750937759876\n",
            "P(w=བདེ་ཆེན་|h=བདེ་ཆེན་ བདེ་ཆེན་ བདེ་ཆེན་ བདེ་ཆེན་ བདེ་ཆེན་)=0.008753247559070587\n",
            "Prob. sentence: 2.226335352696179e-14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6I9HeL0Do6-R"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}