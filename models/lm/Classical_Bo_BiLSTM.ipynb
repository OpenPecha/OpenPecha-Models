{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aa3117e1-4b6e-435b-94dc-08d3c6f2e678",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 07:35:46.389342: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-22 07:35:46.389376: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from typing import List, Dict\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional\n",
    "from tensorflow.keras.models import Sequential\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2e40d1c-1588-4e91-aa17-0004dff9d1de",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"BiLSTM\"\n",
    "model_path = config.MODELS_DIR / model_name\n",
    "model_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "corpus_path = config.DATA_DIR / \"classical_bo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b405304f-beca-41a4-9bd1-d1fbfe3be8cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "803ed750-66c8-4703-96bc-8cf2a427aeb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_paths(path) -> List[str]:\n",
    "   files = []\n",
    "   for pecha_path in tqdm(list(path.iterdir())):\n",
    "     for fn in pecha_path.iterdir():\n",
    "       if 'tokenized' not in fn.stem:\n",
    "         continue\n",
    "       files.append(fn)\n",
    "   return files\n",
    "\n",
    "def normalize_sentences_length(sentences, sent_len=50):\n",
    "  tokens = [tok for sent in sentences for tok in sent.split()]\n",
    "  sentences = []\n",
    "  for i in range(0, len(tokens), sent_len):\n",
    "    sentences.append(tokens[i: i+sent_len])\n",
    "  return [' '.join(s) for s in sentences]\n",
    "   \n",
    "def get_sentences(path, build=False):\n",
    "  sentences_fn = path / \"sentences.txt\"\n",
    "  if sentences_fn.is_file() and not build:\n",
    "    print(\"[INFO] loading sentences from last built...\")\n",
    "    for line in tqdm(sentences_fn.read_text().splitlines()):\n",
    "      if not line: continue\n",
    "      yield line\n",
    "  else:\n",
    "    print(\"[INFO] Building sentences.txt...\")\n",
    "    if sentences_fn.is_file(): sentences_fn.unlink()\n",
    "    sentences = []\n",
    "    for path in tqdm(get_text_paths(path)):\n",
    "      for line in path.read_text().splitlines():\n",
    "        if not line: continue\n",
    "        sentences.append(line)\n",
    "    sentences = normalize_sentences_length(sentences)\n",
    "    sentences_fn.write_text('\\n'.join(sentences))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14cb6f35-24b5-45b5-b3f5-bdcf9d8f602f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "\n",
    "  def __init__(self, seqs, batch_size, vocab_size, shuffle=True):\n",
    "    self.X, self.Y = [], []\n",
    "    self.create_examples(seqs)\n",
    "    self.batch_size = batch_size\n",
    "    self.vocab_size = vocab_size\n",
    "    \n",
    "  @staticmethod\n",
    "  def generate_xy_pairs(seq, max_len):\n",
    "    x, y = [], []\n",
    "    for i, tok_id in enumerate(seq):\n",
    "      x_padded = pad_sequences([seq[:i]], maxlen=max_len)[0]\n",
    "      x.append(x_padded)\n",
    "      y.append(tok_id)\n",
    "    return x, y\n",
    "\n",
    "  def create_examples(self, seqs):\n",
    "    max_len = max([len(seq) for seq in seqs])\n",
    "    for seq in tqdm(seqs):\n",
    "      xs, ys = self.generate_xy_pairs(seq, max_len)\n",
    "      self.X += xs\n",
    "      self.Y += ys\n",
    "\n",
    "    self.X = np.array(self.X)\n",
    "    self.Y = np.array(self.Y)\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X) // self.batch_size\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    start = index*self.batch_size\n",
    "    end = (index+1)*self.batch_size\n",
    "    X = self.X[start: end]\n",
    "    Y = self.Y[start: end]\n",
    "\n",
    "    return np.array(X), to_categorical(Y, num_classes=self.vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e9ca2e-8d47-4c34-a7fe-55daf165cf57",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d10abde4-8ff8-4a18-9a70-2aeaf7631f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(params):\n",
    "  model = Sequential()\n",
    "  model.add(Embedding(input_dim=params[\"vocab_size\"], output_dim=100, input_length=params[\"max_len\"]))\n",
    "  model.add(Bidirectional(LSTM(100, return_sequences=True)))\n",
    "  model.add(Bidirectional(LSTM(100)))\n",
    "  model.add(Dense(params[\"vocab_size\"], activation='softmax'))\n",
    "  model.compile('rmsprop', 'categorical_crossentropy')\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ee2afcf-a6e8-471a-950f-cccb1cb6d284",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-22 07:35:52.411126: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-22 07:35:52.411164: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-22 07:35:52.411196: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (default): /proc/driver/nvidia/version does not exist\n",
      "2022-04-22 07:35:52.412449: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 50, 100)           2000000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 50, 200)          160800    \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, 200)              240800    \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense (Dense)               (None, 20000)             4020000   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 6,421,600\n",
      "Trainable params: 6,421,600\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model({\"vocab_size\": 20000, \"max_len\": 50})\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b1c317b-f0ae-460f-a487-c37d15b04b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(path, model, tokenizer=None):\n",
    "  model.save(path)\n",
    "  if tokenizer:\n",
    "    pickle.dump(tokenizer, (path / 'tokenizer.pkl').open('wb'))\n",
    "  return path\n",
    "\n",
    "def load_model(path):\n",
    "  model = tf.keras.models.load_model(path)\n",
    "  tokenizer = pickle.load((path / 'tokenizer.pkl').open('rb'))\n",
    "  return model, tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8addc8f-749a-4468-acbe-84fd17c313fb",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6abb7539-f96c-4fc9-b2f3-55296637a1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(corpus_path, model_path):\n",
    "  # get setences\n",
    "  print(\"[INFO] Loading sentences...\")\n",
    "  sentences = list(get_sentences(corpus_path))\n",
    "  print(\"[INFO] Loaded no. of sentences (of length 50):\", len(sentences))\n",
    "\n",
    "  # Tokenize\n",
    "  print(\"[INFO] Tokenizing sentences...\")\n",
    "  tokenizer = Tokenizer(lower=False)\n",
    "  tokenizer.fit_on_texts(sentences)\n",
    "  vocab = tokenizer.word_index\n",
    "  seqs = tokenizer.texts_to_sequences_generator(sentences)\n",
    "  max_len = max([len(s.split()) for s in sentences])\n",
    "  del sentences\n",
    "\n",
    "  # Define Parameters\n",
    "  params = {\n",
    "    \"batch_size\": 500,\n",
    "    \"vocab_size\": len(vocab) + 1,\n",
    "    \"shuffle\": True,\n",
    "    \"max_len\": max_len\n",
    "  }\n",
    "\n",
    "  # Create Dataset\n",
    "  print(\"[INFO] Preparing training dataset...\")\n",
    "  train, valid = train_test_split(list(seqs), test_size=0.2, random_state=42)\n",
    "  training_generator = DataGenerator(train, params[\"batch_size\"], params[\"vocab_size\"], params[\"shuffle\"])\n",
    "  validation_generator = DataGenerator(valid, params[\"batch_size\"], params[\"vocab_size\"], params[\"shuffle\"])\n",
    "\n",
    "\n",
    "  # Train model\n",
    "  print(\"[INFO] Training model...\")\n",
    "  model = get_model(params)\n",
    "  model.fit_generator(\n",
    "      generator=training_generator,\n",
    "      validation_data=validation_generator,\n",
    "      epochs=10,\n",
    "      use_multiprocessing=True,\n",
    "      workers=6\n",
    "  )\n",
    "\n",
    "  print(f\"[INFO] Model saved at: {model_path}\")\n",
    "  save_model(model_path, model, tokenizer)\n",
    "\n",
    "  return model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a046ef1d-3713-4b19-a936-62ca4f6a9ef3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loading sentences...\n",
      "[INFO] loading sentences from last built...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 362132/362132 [00:00<00:00, 2899550.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded no. of sentences (of length 50): 362132\n",
      "[INFO] Tokenizing sentences...\n",
      "[INFO] Preparing training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 265164/289705 [04:00<1:31:31,  4.47it/s]"
     ]
    }
   ],
   "source": [
    "train(corpus_path, model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40615c4d-a9d5-4f96-9fc7-cb0a9d0b8bfe",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a66c13d-29b0-4844-a148-5fd98b9aea8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdc0a698-83ee-491c-9a55-7d8cc5350ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_sentence(sentence, model, tokenizer):\n",
    "  seq = tokenizer.texts_to_sequences([sentence])[0]\n",
    "  x_test, y_test = DataGenerator.generate_xy_pairs(seq, model.layers[0].input_length)\n",
    "  x_test = np.array(x_test)\n",
    "  y_test = np.array(y_test)\n",
    "  p_pred = model.predict(x_test)\n",
    "  vocab_inv = {v: k for k, v in tokenizer.word_index.items()}\n",
    "  log_p_sentence = 0\n",
    "  for i, prob in enumerate(p_pred):\n",
    "      word = vocab_inv[y_test[i]] \n",
    "      history = ' '.join([vocab_inv[w] for w in x_test[i, :] if w != 0])\n",
    "      prob_word = prob[y_test[i]]\n",
    "      log_p_sentence += np.log(prob_word)\n",
    "      print('P(w={}|h={})={}'.format(word, history, prob_word))\n",
    "  print('Prob. sentence: {}'.format(np.exp(log_p_sentence)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b27704-b57f-43f0-8b03-c0c80987b024",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
