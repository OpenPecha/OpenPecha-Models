{"metadata":{"jupytext":{"main_language":"python","text_representation":{"extension":".py","format_name":"percent"}},"language_info":{"name":""},"kernelspec":{"name":"","display_name":""}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install comet_ml\n!pip install seqeval[gpu]\n!pip install git+https://www.github.com/keras-team/keras-contrib.git","metadata":{"title":"In [1]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Download word2vec embedding\n!wget https://raw.githubusercontent.com/Esukhia/NER/master/ner_tagger/embeddings/bo_word2vec_v1?token=AD3KLUEFS7ORNGDC2I3FAIS5NTO2S\n!mv bo_word2vec_v1\\?token\\=AD3KLUEFS7ORNGDC2I3FAIS5NTO2S bo_word2vec_v1\n!ls","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comet Experiment Setup\nfrom comet_ml import Experiment","metadata":{"title":"In [2]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_name = 'Bi-LSTM_CRF_Word2Vec'\nversion = 3\ndataset_name = 'citation'\nexp_name = f'{model_name}_v{version}'","metadata":{"title":"In [3]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%writefile .env\nCOMET_API_KEY=vIyyGJwVBzI3hxt2layIDbyye","metadata":{"title":"In [4]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp = Experiment(project_name=\"ner-citation-model\", auto_output_logging='simple')\nexp.set_name(exp_name)","metadata":{"title":"In [5]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Dataset\nimport pandas as pd\nimport numpy as np\nfrom gensim.models import KeyedVectors","metadata":{"title":"In [6]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"WV = KeyedVectors.load_word2vec_format('bo_word2vec_v1', binary=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(f\"../input/ner-lists-87_citations-1000_{dataset_name}.csv\")\ndata = data.fillna(method=\"ffill\")\ndata.tail(10)","metadata":{"title":"In [7]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"words = list(set(data[\"word\"].values))\nn_words = len(words)\nprint(\"# words:\", n_words)","metadata":{"title":"In [8]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tags = list(set(data[\"tag\"].values))\ntags.append('PAD')\nn_tags = len(tags)\nprint('# tags:', n_tags)","metadata":{"title":"In [9]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class SentenceGetter(object):\n    def __init__(self, data):\n        self.n_sent = 1\n        self.data = data\n        self.empty = False\n        agg_func = lambda s: [(w, t) for w, t in zip(s[\"word\"].values.tolist(), s[\"tag\"].values.tolist())]\n        self.grouped = self.data.groupby(\"sentence_idx\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n        \n    def get_next(self):\n        try:\n            s = self.grouped[self.n_sent]\n            self.n_sent += 1\n            return s\n        except:\n            return None","metadata":{"title":"In [10]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"getter = SentenceGetter(data)","metadata":{"title":"In [11]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sentences = getter.sentences","metadata":{"title":"In [12]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")","metadata":{"title":"In [13]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.hist([len(s) for s in sentences], bins=50)\nplt.xlabel('Sentence Length')\nplt.ylabel('No. Sentences')\nexp.log_figure('Sentence length distribution', plt)","metadata":{"title":"In [14]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_len = 150\nword2idx = {}\nword2idx['<UNK>'] = len(WV.vocab)\nword2idx['<PAD>'] = len(WV.vocab) + 1\nfor w in words:\n    if WV.vocab.get(w): word2idx[w] = WV.vocab.get(w).index\n    else: word2idx[w] = word2idx['<UNK>']\nidx2word = {i: t for t, i in word2idx.items()}\ntag2idx = {t: i for i, t in enumerate(tags)}\nidx2tag = {i: t for t, i in tag2idx.items()}","metadata":{"title":"In [16]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical","metadata":{"title":"In [17]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = [[word2idx[w[0]] for w in s] for s in sentences]\nX = pad_sequences(maxlen=max_len, sequences=X, padding=\"post\", value=word2idx['<PAD>'])","metadata":{"title":"In [18]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = [[tag2idx[w[1]] for w in s] for s in sentences]\ny = pad_sequences(maxlen=max_len, sequences=y, padding=\"post\", value=tag2idx[\"PAD\"])\ny = np.array([to_categorical(i, num_classes=n_tags) for i in y])","metadata":{"title":"In [19]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"title":"In [21]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_state = 45\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=random_state)\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=random_state)\nprint('No. Training dataset:', X_train.shape[0])\nprint('No. Validation dataset:', X_valid.shape[0])\nprint('No. Test dataset:', X_test.shape[0])","metadata":{"title":"In [22]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Definitaion -> Bi-LSTM\nfrom keras import backend as K\nfrom keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nfrom keras_contrib.layers import CRF\nfrom keras_contrib.losses.crf_losses import crf_loss\nfrom keras.callbacks import Callback\nfrom seqeval.metrics import f1_score, classification_report, precision_score, recall_score","metadata":{"title":"In [23]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"params = {\n    #dataset\n    'n_train': X_train.shape[0],\n    \n    # sizes hps\n    'vocab_size': len(WV.vocab) + 2, #<UNK> and <PAD>\n    'max_len': max_len,\n    'num_classes': n_tags,\n    'embedding_size': 150,\n    \n    # models hps\n    'optimizer': 'adam',\n    'model_type': model_name,\n    'lstm_layer_1_units': 100,\n    'dense_layer_units': 50,\n    'dense_layer_activation': 'relu',\n    'dropout': 0.1,\n    'recurrent_dropout': 0.1,\n    \n    # training hps\n    'batch_size': 32,\n    'epochs': 50\n}\n\nexp.log_parameters(params)","metadata":{"title":"In [24]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Initialize the embeddings with word2vec\nembedding_matrix = np.random.rand(params['vocab_size'], params['embedding_size']).astype('float32')\nfor word, i in word2idx.items():\n    if word in WV.vocab:\n        embedding_matrix[i] = WV[word]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(WV.vocab), embedding_matrix.shape, embedding_matrix.dtype","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"assert np.array_equal(WV['དང་'], embedding_matrix[WV.vocab.get('དང་').index])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input = Input(shape=(params['max_len'],))\nmodel = Embedding(input_dim=params['vocab_size'], output_dim=params['embedding_size'], input_length=params['max_len'],\n                  mask_zero=False, weights=[embedding_matrix], trainable=False)(input)\nmodel = Dropout(params['dropout'])(model)\nmodel = Bidirectional(LSTM(units=params['lstm_layer_1_units'], return_sequences=True, recurrent_dropout=params['recurrent_dropout']))(model)\nmodel = TimeDistributed(Dense(params['dense_layer_units'], activation=params['dense_layer_activation']))(model)\ncrf = CRF(params['num_classes'])  # CRF layer\nout = crf(model)","metadata":{"title":"In [25]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = Model(input, out)\nmodel.summary()","metadata":{"title":"In [26]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.compile(optimizer=params['optimizer'], loss=crf_loss)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training\nclass F1Metrics(Callback):\n\n    def __init__(self, id2label, pad_value=0, validation_data=None, digits=4):\n        \"\"\"\n        Args:\n            id2label (dict): id to label mapping.\n            (e.g. {1: 'B-LOC', 2: 'I-LOC'})\n            pad_value (int): padding value.\n            digits (int or None): number of digits in printed classification report\n              (use None to print only F1 score without a report).\n        \"\"\"\n        super(F1Metrics, self).__init__()\n        self.id2label = id2label\n        self.pad_value = pad_value\n        self.validation_data = validation_data\n        self.digits = digits\n        self.is_fit = validation_data is None\n\n    def convert_idx_to_name(self, y, array_indexes):\n        \"\"\"Convert label index to name.\n        Args:\n            y (np.ndarray): label index 2d array.\n            array_indexes (list): list of valid index arrays for each row.\n        Returns:\n            y: label name list.\n        \"\"\"\n        y = [[self.id2label[idx] for idx in row[row_indexes]] for\n             row, row_indexes in zip(y, array_indexes)]\n        return y\n\n    def predict(self, X, y):\n        \"\"\"Predict sequences.\n        Args:\n            X (np.ndarray): input data.\n            y (np.ndarray): tags.\n        Returns:\n            y_true: true sequences.\n            y_pred: predicted sequences.\n        \"\"\"\n        y_pred = self.model.predict_on_batch(X)\n\n        # reduce dimension.\n        y_true = np.argmax(y, -1)\n        y_pred = np.argmax(y_pred, -1)\n\n        non_pad_indexes = [np.nonzero(y_true_row != self.pad_value)[0] for y_true_row in y_true]\n\n        y_true = self.convert_idx_to_name(y_true, non_pad_indexes)\n        y_pred = self.convert_idx_to_name(y_pred, non_pad_indexes)\n\n        return y_true, y_pred\n\n    def score(self, y_true, y_pred):\n        \"\"\"Calculate f1 score.\n        Args:\n            y_true (list): true sequences.\n            y_pred (list): predicted sequences.\n        Returns:\n            score: f1 score.\n        \"\"\"\n        score = f1_score(y_true, y_pred)\n        print(' - valid_f1: {:04.2f}'.format(score * 100))\n        return score\n\n    def on_epoch_end(self, epoch, logs={}):\n        if self.is_fit:\n            self.on_epoch_end_fit(epoch, logs)\n        else:\n            self.on_epoch_end_fit_generator(epoch, logs)\n\n    def on_epoch_end_fit(self, epoch, logs={}):\n        X = self.validation_data[0]\n        y = self.validation_data[1]\n        y_true, y_pred = self.predict(X, y)\n        score = self.score(y_true, y_pred)\n        logs['valid_f1'] = score\n\n    def on_epoch_end_fit_generator(self, epoch, logs={}):\n        y_true = []\n        y_pred = []\n        for X, y in self.validation_data:\n            y_true_batch, y_pred_batch = self.predict(X, y)\n            y_true.extend(y_true_batch)\n            y_pred.extend(y_pred_batch)\n        score = self.score(y_true, y_pred)\n        logs['valid_f1'] = score","metadata":{"title":"In [27]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f1_metrics = F1Metrics(idx2tag, tag2idx['PAD'])","metadata":{"title":"In [28]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with exp.train():\n    history = model.fit(X_train, y_train, \n                        batch_size=params['batch_size'], \n                        epochs=params['epochs'], \n                        validation_data=(X_valid, y_valid),\n                        callbacks=[f1_metrics],\n                        verbose=1)","metadata":{"title":"In [31]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate\ndef to_char(x, y, pred):\n    for x, y, p in zip([idx2word[x] for x in x], [idx2tag[x] for x in y], [idx2tag[x] for x in pred]):\n        print(x, y, p)\n\ndef evaluate(X_test, y_test):\n    y_pred = model.predict(X_test)\n    \n    # reduce dimension.\n    y_true = np.argmax(y_test, -1)\n    y_pred = np.argmax(y_pred, -1)\n    \n    #to_char(X_test[1], y_true[1], y_pred[1]) \n    \n    # remove PAD labels\n    non_pad_indexes = [np.nonzero(y_true_row != tag2idx['PAD'])[0] for y_true_row in y_true]\n    y_true = f1_metrics.convert_idx_to_name(y_true, non_pad_indexes)\n    y_pred = f1_metrics.convert_idx_to_name(y_pred, non_pad_indexes)\n    \n    # compute f1 score\n    f1 = f1_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    print(classification_report(y_true, y_pred))\n    return f1, precision, recall, y_pred","metadata":{"title":"In [32]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with exp.test():\n    f1, precision, recall, y_pred = evaluate(X_test, y_test)\n    metrics = {\n        'f1': '{:04.2f}'.format(f1 * 100),\n        'precision': '{:04.2f}'.format(precision * 100),\n        'recall': '{:04.2f}'.format(recall * 100),\n    }\n    exp.log_metrics(metrics)","metadata":{"title":"In [37]:","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#save model\nmodel_fn = f'{exp_name}.h5'\nmodel.save(model_fn)\nexp.log_asset(file_data=model_fn, file_name=model_fn)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def show(i):\n    p = model.predict(np.array([X_test[i]]))\n    p = np.argmax(p, axis=-1)\n    true = np.argmax(y_test[i], -1)\n    print(\"{:15}||{:5}||{}\".format(\"Word\", \"True\", \"Pred\"))\n    print(30 * \"=\")\n    for w, t, pred in zip(X_test[i], true, p[0]):\n        if idx2word[w] != '<PAD>':\n            print(\"{:15}: {:5} {}\".format(idx2word[w], idx2tag[t], idx2tag[pred]))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show(20)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show(32)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"show(50)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"exp.end()","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}