{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "from pathlib import Path\n",
    "\n",
    "import logging  # Setting up the loggings to monitor gensim\n",
    "logging.basicConfig(format=\"%(levelname)s - %(asctime)s: %(message)s\", datefmt= '%H:%M:%S', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_fn = Path('../input/tokenized_lemmatized_paragraphs.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_paras = [para.split(' ') for para in data_fn.read_text().split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ན་མོ་',\n",
       " 'གུ་རུ་',\n",
       " 'དེ་བ་',\n",
       " 'ཌཱ་ཀི་',\n",
       " 'ནཱི་',\n",
       " 'ཡཻ',\n",
       " '།_',\n",
       " 'དགོངས་པ་',\n",
       " 'གི་',\n",
       " 'སྟོབས་',\n",
       " 'དང་',\n",
       " 'ཚུལ་ལྡན་',\n",
       " 'ཆོ་ག་',\n",
       " 'གི་',\n",
       " 'མཐུ་',\n",
       " 'གིས་',\n",
       " '།_།',\n",
       " 'ཐོག་མེད་',\n",
       " 'འཁྲུལ་པ་',\n",
       " 'གི་',\n",
       " 'འཆིང་བ་',\n",
       " 'སྐད་ཅིག་',\n",
       " 'ལ་',\n",
       " '།_།',\n",
       " 'བྲལ་',\n",
       " 'ན་',\n",
       " 'མངོན་སུམ་',\n",
       " 'ཡེ་ཤེས་',\n",
       " 'སད་',\n",
       " 'མཛད་པ་',\n",
       " '།_།',\n",
       " 'དཀྱིལ་འཁོར་',\n",
       " 'དབང་ཕྱུག་',\n",
       " 'དཔལ་ལྡན་',\n",
       " 'བླ་མ་',\n",
       " 'ལ་',\n",
       " 'འདུད་',\n",
       " '།_།',\n",
       " 'རྡོ་རྗེ་',\n",
       " 'ཐེག་པ་',\n",
       " 'གི་',\n",
       " 'རྩ་བ་',\n",
       " 'སྨིན་',\n",
       " 'བྱེད་',\n",
       " 'ཀྱི་',\n",
       " '།_།',\n",
       " 'ཚུལ་',\n",
       " 'འདི་',\n",
       " 'ཟབ་',\n",
       " 'རྒྱ་',\n",
       " 'ཉིད་',\n",
       " 'ཕྱི་',\n",
       " 'རྟོགས་དཀའ་',\n",
       " 'ཡང་',\n",
       " '།_།',\n",
       " 'དང་པོ་',\n",
       " 'གི་',\n",
       " 'ལས་ཅན་',\n",
       " 'ཕྱོགས་',\n",
       " 'ཙམ་',\n",
       " 'ངེས་',\n",
       " 'རྙེད་',\n",
       " 'ཕྱི་',\n",
       " '།_།',\n",
       " 'གོ་',\n",
       " 'བདེ་',\n",
       " 'གི་',\n",
       " 'ངག་',\n",
       " 'གིས་',\n",
       " 'མདོར་བསྡུས་',\n",
       " 'བརྗོད་པ་',\n",
       " 'ལ་',\n",
       " 'བྱ་',\n",
       " '།_།',\n",
       " 'དེ་',\n",
       " 'ཀྱང་',\n",
       " 'རྡོ་རྗེ་',\n",
       " 'ཐེག་པ་',\n",
       " 'གི་',\n",
       " 'ལམ་',\n",
       " 'གྱི་',\n",
       " 'གནད་',\n",
       " 'ཐམས་ཅད་',\n",
       " 'ཚང་',\n",
       " 'ཞིང་',\n",
       " 'ཁྱད་པར་',\n",
       " 'གསང་སྔགས་',\n",
       " 'ཀྱི་',\n",
       " 'རྒྱུད་',\n",
       " 'ལུང་',\n",
       " 'མན་ངག་',\n",
       " 'རྣམས་',\n",
       " 'ལ་',\n",
       " 'ཐོས་བསམ་',\n",
       " 'སྒོམ་པ་',\n",
       " 'གང་',\n",
       " 'བྱེད་',\n",
       " 'ཀྱང་',\n",
       " 'ངེས་པ་',\n",
       " 'ལ་',\n",
       " 'སྔོན་',\n",
       " 'དུ་མ་',\n",
       " 'སོང་',\n",
       " 'ཐབས་མེད་པ་',\n",
       " 'ནི་',\n",
       " 'དབང་',\n",
       " 'བསྐུར་བ་',\n",
       " 'དང་',\n",
       " '།_',\n",
       " 'དེ་ལས་',\n",
       " 'ཐོབ་པ་',\n",
       " 'གི་',\n",
       " 'དམ་ཚིག་',\n",
       " 'མི་',\n",
       " 'ཉམས་པ་',\n",
       " 'ལ་',\n",
       " 'སྲུང་བ་',\n",
       " 'ཁོ་ན་',\n",
       " 'ཡིན་',\n",
       " 'ཞིང་',\n",
       " '།_',\n",
       " 'དབང་མ་',\n",
       " 'ནོས་',\n",
       " 'པ་',\n",
       " 'གསང་སྔགས་',\n",
       " 'ལ་',\n",
       " 'ཞུགས་',\n",
       " 'ན་',\n",
       " 'དངོས་གྲུབ་',\n",
       " 'མི་',\n",
       " 'འགྲུབ་པ་',\n",
       " 'ལ་',\n",
       " 'མ་ཟད་',\n",
       " 'འདི་',\n",
       " 'དང་',\n",
       " 'གཏན་',\n",
       " 'ཏུ་',\n",
       " 'ཉམས་པ་',\n",
       " 'ཆེན་པོ་',\n",
       " 'ལ་',\n",
       " 'འགྱུར་བ་',\n",
       " 'གིས་',\n",
       " '།']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_paras[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48539"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_freq = defaultdict(int)\n",
    "for para in tokenized_paras:\n",
    "    for i in para:\n",
    "        word_freq[i] += 1\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ལ་', 'གི་', '།_', '།_།', 'དང་', 'ན་', 'གིས་', 'དུ་', 'ཀྱི་', 'དེ་']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(word_freq, key=word_freq.get, reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why I seperate the training of the model in 3 steps:\n",
    "I prefer to separate the training in 3 distinctive steps for clarity and monitoring.\n",
    "1. `Word2Vec()`: \n",
    ">In this first step, I set up the parameters of the model one-by-one. <br>I do not supply the parameter `sentences`, and therefore leave the model uninitialized, purposefully.\n",
    "2. `.build_vocab()`: \n",
    ">Here it builds the vocabulary from a sequence of sentences and thus initialized the model. <br>With the loggings, I can follow the progress and even more important, the effect of `min_count` and `sample` on the word corpus. I noticed that these two parameters, and in particular `sample`, have a great influence over the performance of a model. Displaying both allows for a more accurate and an easier management of their influence.\n",
    "3. `.train()`:\n",
    ">Finally, trains the model.<br>\n",
    "The loggings here are mainly useful for monitoring, making sure that no threads are executed instantaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The parameters:\n",
    "\n",
    "* `min_count` <font color='purple'>=</font> <font color='green'>int</font> - Ignores all words with total absolute frequency lower than this - (2, 100)\n",
    "\n",
    "\n",
    "* `window` <font color='purple'>=</font> <font color='green'>int</font> - The maximum distance between the current and predicted word within a sentence. E.g. `window` words on the left and `window` words on the left of our target - (2, 10)\n",
    "\n",
    "\n",
    "* `size` <font color='purple'>=</font> <font color='green'>int</font> - Dimensionality of the feature vectors. - (50, 300)\n",
    "\n",
    "\n",
    "* `sample` <font color='purple'>=</font> <font color='green'>float</font> - The threshold for configuring which higher-frequency words are randomly downsampled. Highly influencial.  - (0, 1e-5)\n",
    "\n",
    "\n",
    "* `alpha` <font color='purple'>=</font> <font color='green'>float</font> - The initial learning rate - (0.01, 0.05)\n",
    "\n",
    "\n",
    "* `min_alpha` <font color='purple'>=</font> <font color='green'>float</font> - Learning rate will linearly drop to `min_alpha` as training progresses. To set it: alpha - (min_alpha * epochs) ~ 0.00\n",
    "\n",
    "\n",
    "* `negative` <font color='purple'>=</font> <font color='green'>int</font> - If > 0, negative sampling will be used, the int for negative specifies how many \"noise words\" should be drown. If set to 0, no negative sampling is used. - (5, 20)\n",
    "\n",
    "\n",
    "* `workers` <font color='purple'>=</font> <font color='green'>int</font> - Use these many worker threads to train the model (=faster training with multicore machines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension of word embedding\n",
    "The optimal dimensionality of word embeddings is mostly task-dependent: a smaller dimensionality works better for more syntactic tasks such as named entity recognition (Melamud et al., 2016) [3] or part-of-speech (POS) tagging (Plank et al., 2016) [4], while a larger dimensionality is more useful for more semantic tasks such as sentiment analysis (Ruder et al., 2016) [5].\n",
    "\n",
    "- [3] -> http://arxiv.org/abs/1601.00893\n",
    "- [4] -> Plank, B., Søgaard, A., & Goldberg, Y. (2016). Multilingual Part-of-Speech Tagging with Bidirectional Long Short-Term Memory Models and Auxiliary Loss. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics. \n",
    "- [5] -> http://arxiv.org/abs/1609.02745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(min_count=20,\n",
    "                     window=5,\n",
    "                     size=150,\n",
    "                     sample=6e-5, \n",
    "                     alpha=0.03, \n",
    "                     min_alpha=0.0007, \n",
    "                     negative=20,\n",
    "                     workers=cores-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Vocabulary Table:\n",
    "Word2Vec requires us to build the vocabulary table (simply digesting all the words and filtering out the unique words, and doing some basic counts on them):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to build vocab: 0.11 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.build_vocab(tokenized_paras, progress_per=10000)\n",
    "\n",
    "print('Time to build vocab: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training of the model:\n",
    "_Parameters of the training:_\n",
    "* `total_examples` <font color='purple'>=</font> <font color='green'>int</font> - Count of sentences;\n",
    "* `epochs` <font color='purple'>=</font> <font color='green'>int</font> - Number of iterations (epochs) over the corpus - [10, 20, 30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to train the model: 5.6 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "\n",
    "w2v_model.train(tokenized_paras, total_examples=w2v_model.corpus_count, epochs=30, report_delay=1)\n",
    "\n",
    "print('Time to train the model: {} mins'.format(round((time() - t) / 60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('མཐུ་', 0.608696699142456),\n",
       " ('ནུས་སྟོབས་', 0.45071011781692505),\n",
       " ('ཤུགས་', 0.4431358575820923),\n",
       " ('གོམས་པ་', 0.4223802387714386),\n",
       " ('སོར་ཆུད་པ་', 0.42098259925842285),\n",
       " ('ཡོན་ཏན་', 0.4176880419254303),\n",
       " ('སྐྱེས་ཐོབ་', 0.4152085483074188),\n",
       " ('ལྡན་པ་', 0.41036728024482727),\n",
       " ('མཐུ་ཆེན་པོ་', 0.4098166823387146),\n",
       " ('བརྩོན་འགྲུས་', 0.4016813039779663)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"སྟོབས་\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('མཛད་', 0.7612118721008301),\n",
       " ('ཕྲིན་ལས་', 0.5403375029563904),\n",
       " ('མཛད་པ་པོ་', 0.49853041768074036),\n",
       " ('སྟོན་པ་', 0.4814871549606323),\n",
       " ('འགྲེལ་པ་', 0.47240862250328064),\n",
       " ('སྒྲུབ་ཐབས་', 0.4716480076313019),\n",
       " ('གྲགས་པ་', 0.46867769956588745),\n",
       " ('གདུལ་བྱ་', 0.4681282937526703),\n",
       " ('བསྟན་པ་', 0.4617062211036682),\n",
       " ('འཕྲིན་ལས་', 0.4564966857433319)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"མཛད་པ་\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('བྱིན་རླབས་', 0.6062471866607666),\n",
       " ('མོས་གུས་', 0.564081072807312),\n",
       " ('མཚན་ལྡན་', 0.5319912433624268),\n",
       " ('རྗེ་', 0.5182524919509888),\n",
       " ('བཀའ་དྲིན་', 0.5181214809417725),\n",
       " ('བརྒྱུད་པ་', 0.5165523886680603),\n",
       " ('ཡི་དམ་', 0.5100193023681641),\n",
       " ('དད་གུས་', 0.5081140995025635),\n",
       " ('དྲིན་ཅན་', 0.4949682950973511),\n",
       " ('སྐྱབས་གནས་', 0.48522794246673584)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"བླ་མ་\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('ལ་རྩ་བ་', 0.504997730255127),\n",
       " ('མདོར་ན་', 0.40671637654304504),\n",
       " ('ཐམས་ཅད་', 0.37312084436416626),\n",
       " ('བསྐྱེད་པ་', 0.36299213767051697),\n",
       " ('རྟེན་གཞི་', 0.3564617335796356),\n",
       " ('རྒྱུད་', 0.35559332370758057),\n",
       " ('སྡོམ་པ་', 0.35458317399024963),\n",
       " ('གཙོ་བོ་', 0.35075682401657104),\n",
       " ('གསུམ་པོ་', 0.3506447374820709),\n",
       " ('སྤྱིར་', 0.3458601236343384)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"རྩ་བ་\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('དེ་ཉིད་', 0.784591794013977),\n",
       " ('རང་བཞིན་', 0.6753857731819153),\n",
       " ('ངོ་བོ་', 0.6689523458480835),\n",
       " ('བདག་ཉིད་', 0.6479246616363525),\n",
       " ('ནི་', 0.636000394821167),\n",
       " ('ཕྱི་', 0.6137641668319702),\n",
       " ('དེ་བཞིན་ཉིད་', 0.5992171764373779),\n",
       " ('ནོ་', 0.5715460181236267),\n",
       " ('ཞེ་', 0.5701719522476196),\n",
       " ('གི་', 0.5472181439399719)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar(positive=[\"ཉིད་\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model.wv.save_word2vec_format(\"./bo_word2vec_lammatized\",\n",
    "                              \"./vocab\",\n",
    "                               binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__notebook__.ipynb  __output__.json  bo_word2vec_lammatized  vocab\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv_from_text = KeyedVectors.load_word2vec_format('bo_word2vec_lammatized', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('དེ་ཉིད་', 0.784591794013977),\n",
       " ('རང་བཞིན་', 0.6753857731819153),\n",
       " ('ངོ་བོ་', 0.6689522862434387),\n",
       " ('བདག་ཉིད་', 0.6479246616363525),\n",
       " ('ནི་', 0.636000394821167),\n",
       " ('ཕྱི་', 0.613764226436615),\n",
       " ('དེ་བཞིན་ཉིད་', 0.5992171764373779),\n",
       " ('ནོ་', 0.5715460181236267),\n",
       " ('ཞེ་', 0.5701720118522644),\n",
       " ('གི་', 0.5472180843353271)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv_from_text.wv.most_similar(positive=[\"ཉིད་\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
