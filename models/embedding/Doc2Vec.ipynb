{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e986fde9-6709-4d3d-bdca-d3c323f37e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U gensim -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3f613143-321f-4c0b-b11a-2fc8ef887f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from pathlib import Path\n",
    "import random\n",
    "import unicodedata\n",
    "\n",
    "import gensim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55dcabc9-d850-4cee-97bf-5df83f8dae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfd7f2f-7593-44b2-a73b-c4f99beb88ae",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "451d4694-8f7a-43dd-afd5-ad3f5d9eaed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.home() / \".models\"\n",
    "DATA_DIR = BASE_DIR / \"data\"\n",
    "MODELS_DIR = BASE_DIR / \"models\"\n",
    "\n",
    "corpus_path = DATA_DIR / \"classical_bo\"\n",
    "model_path = str((MODELS_DIR / \"doc2vec_classical_bo\").resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b6550d02-b627-46d7-a1b8-fdf0acd70963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_files(path):\n",
    "    for pecha_path in path.iterdir():\n",
    "        if not pecha_path.is_dir(): continue\n",
    "        for fn in pecha_path.iterdir():\n",
    "            if not 'tokenize' in fn.name: continue\n",
    "            yield fn\n",
    "            \n",
    "def is_punt(word):\n",
    "    for punt in [\"།\", \"།།\", \"༄༅\"]:\n",
    "        if punt in word:\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def tokenize(text):\n",
    "    return [token for token in text.split() if token and not is_punt(token)]\n",
    "    \n",
    "def get_sentences(fns):\n",
    "    for fn in fns:\n",
    "        for sentence in fn.open('r'):\n",
    "            if len(sentence.split()) < 3: continue\n",
    "            yield tokenize(unicodedata.normalize(\"NFKC\", sentence.strip()))\n",
    "\n",
    "def create_dataset(sentences, tokens_only=False):\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        tokens = sentence # already tokenize\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, [i])       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5e8b43e4-601c-4dd0-9571-d44f6387b645",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = get_files(corpus_path)\n",
    "train_files, test_files = train_test_split(list(files), test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "71b5a712-9dae-4fa4-afa8-0772d4f552a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sents = get_sentences(train_files)\n",
    "test_sents = get_sentences(test_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fa7df41c-a857-4482-a66e-ea94af23c084",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus = list(create_dataset(train_sents))\n",
    "test_corpus = list(create_dataset(test_sents, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "297210b3-c61c-4501-bdb1-9be8f480a3db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(495842,\n",
       " [TaggedDocument(words=['བདེ་གཤེགས་', 'སྙིང་པོ', 'འང་', 'འོད་གསལ་བ', 'འི་', 'སེམས་', 'འདི་', 'ལ་', 'བརྗོད་པ་', 'ཡིན་', 'ནོ'], tags=[495842]),\n",
       "  TaggedDocument(words=['རྫོགས་ཆེན་', 'ངེས་དོན་', 'འདུས་པ', 'འི་', 'རྒྱུད་', 'ལ', 'ས', 'སེམས་', 'ལ་དུ', 'ས་', 'གསུམ་', 'ཡོད་པ', 'འི་', 'ཕྱིར', 'ཚེ་', 'ལ་', 'སྔ་ཕྱི་', 'ད་ལྟ་', 'བྱུང་'], tags=[495843]),\n",
       "  TaggedDocument(words=['སེམས་', 'ལ་', 'འཕོ་འགྱུར་', 'ཡོད་པ', 'འི་', 'ཕྱིར', 'དེ་ཕྱིར་', 'ལུས་', 'ལ་', 'སྐྱེ་འཆི་', 'བྱུང་'], tags=[495844])])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "doc_id, train_corpus[doc_id: doc_id+3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3fe981c4-f47d-4b5d-bab7-3149f85cc428",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(train_corpus) == train_corpus[-1].tags[0] + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79d0e25-340f-43db-8192-c9b69d5b9f56",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f908ffe1-de04-4950-8134-69d225a0e676",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 11:19:13,326 : INFO : Doc2Vec lifecycle event {'params': 'Doc2Vec(dm/m,d50,n5,w5,mc2,s0.001,t3)', 'datetime': '2022-04-29T11:19:13.326610', 'gensim': '4.1.2', 'python': '3.9.10 | packaged by conda-forge | (main, Feb  1 2022, 21:24:37) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.262-200.489.amzn2.x86_64-x86_64-with-glibc2.31', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d26fed9-599a-4b1a-be24-1f3606461a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89bf83bb-671b-4e1a-9aa0-013f381b0b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4704393-32a7-4e3c-9467-868fba2483e0",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2baa87cf-abe1-4f47-bf3d-da1be66c5816",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.13087395, -0.01936051, -0.0087766 ,  0.12466373, -0.5885176 ,\n",
       "       -0.24993752,  0.00286173,  0.38588482, -0.01656335,  0.17086115,\n",
       "        0.30103028,  0.33869773, -0.24266653, -0.4151375 , -0.6437144 ,\n",
       "        0.12202694,  0.3668979 , -0.3083544 ,  0.11743522, -0.49491903,\n",
       "        0.05449725,  0.00839138,  0.1312454 , -0.28812018,  0.01965531,\n",
       "        0.53242975, -0.34315082,  0.6922586 , -0.6106972 ,  0.19089296,\n",
       "        0.9451179 , -0.39846855,  0.27619553, -0.8079054 , -0.21874213,\n",
       "        0.4931009 , -0.21541193,  0.48782465, -0.15523456,  0.81462157,\n",
       "       -0.2974267 ,  0.31295067, -0.44193265,  0.24974515,  0.05155938,\n",
       "       -0.03777373, -0.6853228 , -0.39054087, -0.67567915, -0.25841784],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.infer_vector([\"རིམ་པ་\", \"བཞིན་\", \"དུ་\", \"སྦྱིན་པ་\", \"ལ་\", \"བྱ་\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb5ccc4-595a-49ce-a5b1-251e06edd436",
   "metadata": {},
   "source": [
    "## Save and Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "be520858-4a44-4ed1-9a86-fe2fc5b2e97b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 12:07:44,680 : INFO : Doc2Vec lifecycle event {'fname_or_handle': '/home/studio-lab-user/.models/models/doc2vec_classical_bo', 'separately': 'None', 'sep_limit': 10485760, 'ignore': frozenset(), 'datetime': '2022-04-29T12:07:44.680554', 'gensim': '4.1.2', 'python': '3.9.10 | packaged by conda-forge | (main, Feb  1 2022, 21:24:37) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.262-200.489.amzn2.x86_64-x86_64-with-glibc2.31', 'event': 'saving'}\n",
      "2022-04-29 12:07:44,681 : INFO : storing np array 'vectors' to /home/studio-lab-user/.models/models/doc2vec_classical_bo.dv.vectors.npy\n",
      "2022-04-29 12:07:44,751 : INFO : not storing attribute cum_table\n",
      "2022-04-29 12:07:44,842 : INFO : saved /home/studio-lab-user/.models/models/doc2vec_classical_bo\n"
     ]
    }
   ],
   "source": [
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "67113b73-da5d-4852-bc36-10a766da789b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-29 12:07:44,847 : INFO : loading Doc2Vec object from /home/studio-lab-user/.models/models/doc2vec_classical_bo\n",
      "2022-04-29 12:07:44,909 : INFO : loading dv recursively from /home/studio-lab-user/.models/models/doc2vec_classical_bo.dv.* with mmap=None\n",
      "2022-04-29 12:07:44,910 : INFO : loading vectors from /home/studio-lab-user/.models/models/doc2vec_classical_bo.dv.vectors.npy with mmap=None\n",
      "2022-04-29 12:07:44,964 : INFO : loading wv recursively from /home/studio-lab-user/.models/models/doc2vec_classical_bo.wv.* with mmap=None\n",
      "2022-04-29 12:07:44,965 : INFO : setting ignored attribute cum_table to None\n",
      "2022-04-29 12:07:45,326 : INFO : Doc2Vec lifecycle event {'fname': '/home/studio-lab-user/.models/models/doc2vec_classical_bo', 'datetime': '2022-04-29T12:07:45.325962', 'gensim': '4.1.2', 'python': '3.9.10 | packaged by conda-forge | (main, Feb  1 2022, 21:24:37) \\n[GCC 9.4.0]', 'platform': 'Linux-4.14.262-200.489.amzn2.x86_64-x86_64-with-glibc2.31', 'event': 'loaded'}\n"
     ]
    }
   ],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec.load(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0558eb-fb77-4a0c-a192-9af184c0f056",
   "metadata": {},
   "source": [
    "## Assessing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200cf695-621f-4654-b3c0-52c1e62a4e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(train_corpus)):\n",
    "    inferred_vector = model.infer_vector(train_corpus[doc_id].words)\n",
    "    sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27e2293-580c-4f48-bb0a-4cf8189aa799",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "counter = collections.Counter(ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f324752c-417b-4e7e-9a88-7b057de0575a",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_id = 1\n",
    "print('Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213e7845-3f9c-43dd-8bdc-adeb7d9916f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(train_corpus) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(train_corpus[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(train_corpus[sim_id[0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d573a0-e64d-4c51-8f7f-1d65f0ce14ac",
   "metadata": {},
   "source": [
    "## Testing the Model\n",
    "Using the same approach above, we’ll infer the vector for a randomly chosen test document, and compare the document to our model by eye."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9407aa97-8927-45ef-b09f-ec9f23eab144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick a random document from the test corpus and infer a vector from the model\n",
    "doc_id = random.randint(0, len(test_corpus) - 1)\n",
    "inferred_vector = model.infer_vector(test_corpus[doc_id])\n",
    "sims = model.dv.most_similar([inferred_vector], topn=len(model.dv))\n",
    "\n",
    "# Compare and print the most/median/least similar documents from the train corpus\n",
    "print('Test Document ({}): «{}»\\n'.format(doc_id, ' '.join(test_corpus[doc_id])))\n",
    "print(u'SIMILAR/DISSIMILAR DOCS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(train_corpus[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2afe58-2c97-4fbf-a915-98a2ae757f90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
